import requests
import json
from pymongo import MongoClient
from datetime import datetime
import logging

ATLAS_URL = "http://172.26.0.2:21000"  # V√©rifiez que votre Atlas est accessible
ATLAS_USER = "admin"                    # V√©rifiez les credentials
ATLAS_PASS = "ensias123"          # V√©rifiez les credentials

# Configuration MongoDB - √Ä AJUSTER selon votre environnement
MONGO_URI = 'mongodb://mongodb:27017/' 

# Configuration logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AtlasMetadataGovernance:
    def __init__(self):
        self.atlas_url = ATLAS_URL
        self.auth = (ATLAS_USER, ATLAS_PASS)
        self.mongo_client = MongoClient(MONGO_URI)
        self.metadata_db = self.mongo_client['metadata_validation_db']
        
        # Test de connectivit√© initial
        self._test_connections()
        
    def _test_connections(self):
        """Tester les connexions Atlas et MongoDB"""
        logger.info("üîç Test des connexions...")
        
        # Test MongoDB
        try:
            self.mongo_client.admin.command('ping')
            logger.info("‚úÖ MongoDB connect√©")
        except Exception as e:
            logger.error(f"‚ùå MongoDB non accessible: {e}")
            raise
        
        # Test Atlas
        


    def get_hive_table_entity(self, table_name):  
     """R√©cup√©rer l'entit√© table Hive et ses colonnes"""  
     search_url = f"{self.atlas_url}/api/atlas/v2/search/dsl"  
     dsl_query = f"hive_table where name='{table_name}'"  
      
     response = requests.get(  
        search_url,  
        auth=self.auth,  
        params={'query': dsl_query}  
     )  
     logger.info(f"Recherche table {table_name}: {response.status_code}")  

     if response.status_code == 200:  
        entities = response.json().get('entities', [])  
        logger.info(f"Entit√©s trouv√©es: {len(entities)}")  

        if entities:  
            return entities[0]['guid']  
     return None  
  
    def get_table_columns(self, table_guid):  
     """R√©cup√©rer toutes les colonnes de la table"""  
     entity_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{table_guid}"  
      
     response = requests.get(  
        entity_url,  
        auth=self.auth,  
        params={'ignoreRelationships': 'false'}  
     )  
      
     if response.status_code == 200:  
        entity = response.json()['entity']  
        columns = entity.get('relationshipAttributes', {}).get('columns', [])  
          
        column_info = []  
        for col in columns:  
            column_info.append({  
                'guid': col['guid'],  
                'name': col['displayText'],  
                'type': col['typeName']  
            })  
        return column_info  
     return []
    

    def assign_glossary_term_to_column(self, column_guid, term_guid):  
     """Assigner un terme de glossaire √† une colonne Hive"""  
     assign_url = f"{self.atlas_url}/api/atlas/v2/entity/guid/{column_guid}/meanings"  
      
     payload = [{  
        "termGuid": term_guid,  
        "relationGuid": None  
     }]  
      
     response = requests.post(  
        assign_url,  
        auth=self.auth,  
        headers={'Content-Type': 'application/json'},  
        json=payload  
     )  
      

     if response.status_code != 200:  
        logger.error(f"Erreur assignation: {response.status_code} - {response.text}")  
      
     return response.status_code == 200  
  
    def get_glossary_term_guid(self, term_name):  
     """R√©cup√©rer le GUID d'un terme du glossaire"""  
     search_url = f"{self.atlas_url}/api/atlas/v2/search/basic"  
      
     response = requests.get(  
        search_url,  
        auth=self.auth,  
        params={  
            'query': term_name,  
            'typeName': 'AtlasGlossaryTerm',  
            'excludeDeletedEntities': 'true'  
        }  
     )  
     logger.info(f"Recherche terme {term_name}: {response.status_code}")  

     if response.status_code == 200:  
        entities = response.json().get('entities', [])  
        logger.info(f"Termes trouv√©s: {len(entities)}")  
        for entity in entities:  
            if entity['displayText'] == term_name:  
                return entity['guid']  
     return None
    

    def create_column_term_mapping(self):  
     """Cr√©er mapping entre colonnes Hive et termes du glossaire"""  
     # Utiliser vos m√©tadonn√©es valid√©es existantes  
     enriched_metadata = self.metadata_db['enriched_metadata']  
     validated_metadata = list(enriched_metadata.find({"validation_status": "validated"}))  
      
     mapping = {}  
      
     for metadata in validated_metadata:  
        column_name = metadata['column_name']  
        entity_types = metadata.get('entity_types', [])  
        rgpd_category = metadata.get('recommended_rgpd_category')  
          
        # Cr√©er le nom du terme comme dans votre m√©thode create_validated_metadata_terms  
        term_name = f"{column_name.upper()}_TERM"  
          
        mapping[column_name] = term_name  
          
        logger.info(f"Mapping cr√©√©: {column_name} ‚Üí {term_name}")  
      
     return mapping


    def get_glossary_term_guid(self, term_name):  
     """R√©cup√©rer le GUID d'un terme du glossaire avec s√©lection pr√©cise"""  
     search_url = f"{self.atlas_url}/api/atlas/v2/search/basic"  
      
     response = requests.get(  
        search_url,  
        auth=self.auth,  
        params={  
            'query': term_name,  
            'typeName': 'AtlasGlossaryTerm',  
            'excludeDeletedEntities': 'true'  
        }  
     )  
      
     if response.status_code == 200:  
        entities = response.json().get('entities', [])  
          
        # Filtrer par glossaire exact et nom exact  
        for entity in entities:  
            # V√©rifier que le terme appartient au bon glossaire  
            if (entity['displayText'] == term_name and   
                entity.get('attributes', {}).get('anchor', {}).get('glossaryGuid') == self.current_glossary_guid):  
                return entity['guid']  
          
        # Fallback : s√©lection par nom exact seulement  
        for entity in entities:  
            if entity['displayText'] == term_name:  
                return entity['guid']  
      
     return None
    
    def preview_sync_data(self):
        """Pr√©visualiser les donn√©es qui seront synchronis√©es"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        
        # Statistiques
        total_metadata = enriched_metadata.count_documents({})
        validated_metadata = enriched_metadata.count_documents({"validation_status": "validated"})
        pending_metadata = enriched_metadata.count_documents({"validation_status": "pending"})
        
        # Cat√©gories et niveaux uniques
        categories = enriched_metadata.distinct('recommended_rgpd_category')
        sensitivity_levels = enriched_metadata.distinct('recommended_sensitivity_level')
        
        preview = {
            "total_metadata": total_metadata,
            "validated_metadata": validated_metadata,
            "pending_metadata": pending_metadata,
            "rgpd_categories": [cat for cat in categories if cat],
            "sensitivity_levels": [level for level in sensitivity_levels if level],
            "will_sync": validated_metadata > 0
        }
        
        logger.info("üìä PR√âVISUALISATION DE LA SYNCHRONISATION")
        logger.info(f"üìù Total m√©tadonn√©es: {total_metadata}")
        logger.info(f"‚úÖ M√©tadonn√©es valid√©es (√† synchroniser): {validated_metadata}")
        logger.info(f"‚è≥ M√©tadonn√©es en attente: {pending_metadata}")
        logger.info(f"üìÇ Cat√©gories RGPD: {preview['rgpd_categories']}")
        logger.info(f"üîí Niveaux de sensibilit√©: {preview['sensitivity_levels']}")
        
        if validated_metadata == 0:
            logger.warning("‚ö†Ô∏è  AUCUNE M√âTADONN√âE VALID√âE - Rien ne sera synchronis√©!")
            
        return preview
        
        # Test de connectivit√© initial
        self._test_connections()
        
    def create_business_glossary(self):
        """Cr√©er le glossaire m√©tier principal"""
        glossary_data = {
            "name": "Data_Governance_Glossary11",
            "shortDescription": "Glossaire m√©tier pour la gouvernance des donn√©es",
            "longDescription": "Glossaire centralis√© contenant toutes les m√©tadonn√©es valid√©es par les data stewards, enrichies avec les recommandations IA et conformes aux exigences RGPD"
        }
        
        response = requests.post(
            f"{self.atlas_url}/api/atlas/v2/glossary",
            json=glossary_data,
            auth=self.auth,
            timeout=(30, 60)
        )
        
        if response.status_code == 200:  
              glossary_guid = response.json()['guid']  
              self.current_glossary_guid = glossary_guid  # Stocker pour la s√©lection des termes  
              logger.info("‚úì Glossaire m√©tier cr√©√© avec succ√®s")  
              return glossary_guid
        else:
            logger.error(f"‚úó Erreur cr√©ation glossaire: {response.text}")
            return {"success": False, "error": "√âchec "}

    def extract_rgpd_categories_from_db(self):
        """Extraire les cat√©gories RGPD r√©elles depuis la base"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        categories = enriched_metadata.distinct('recommended_rgpd_category')
        categories = [cat for cat in categories if cat and cat.strip()]
        logger.info(f"Cat√©gories RGPD d√©tect√©es: {categories}")
        return categories

    def create_rgpd_categories(self, glossary_guid):
        """Cr√©er les cat√©gories RGPD bas√©es sur les donn√©es r√©elles"""
        real_categories = self.extract_rgpd_categories_from_db()
        category_guids = {}
        
        # Mapping des descriptions m√©tier
        category_descriptions = {
            "Donn√©es d'identification": "Informations permettant d'identifier directement ou indirectement une personne physique",
            "Donn√©es financi√®res": "Informations bancaires, financi√®res et de paiement",
            "Donn√©es de contact": "Informations de contact et de communication",
            "Donn√©es de localisation": "Informations g√©ographiques et d'adresse",
            "Donn√©es temporelles": "Informations de date, heure et temporelles",
            "Donn√©es de sant√©": "Informations m√©dicales et de sant√©",
            "Donn√©es biom√©triques": "Donn√©es biom√©triques d'identification",
            "Donn√©es de comportement": "Donn√©es de navigation et comportementales"
        }
        
        for category in real_categories:
            cat_data = {
                "name": category,
                "shortDescription": category_descriptions.get(category, f"Cat√©gorie RGPD: {category}"),
                "longDescription": f"Cat√©gorie de donn√©es personnelles selon le RGPD: {category}. Gestion automatis√©e avec validation data steward.",
                "anchor": {"glossaryGuid": glossary_guid}
            }
            
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/glossary/category",
                json=cat_data,
                auth=self.auth
            )
            
            if response.status_code == 200:
                category_guids[category] = response.json()['guid']
                logger.info(f"‚úì Cat√©gorie RGPD cr√©√©e: {category}")
            else:
                logger.error(f"‚úó Erreur cat√©gorie {category}: {response.text}")
        
        return category_guids

    def create_sensitivity_classifications(self):
        """Cr√©er les classifications de sensibilit√© bas√©es sur les donn√©es r√©elles"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        sensitivity_levels = enriched_metadata.distinct('recommended_sensitivity_level')
        sensitivity_levels = [level for level in sensitivity_levels if level]
        
        logger.info(f"Niveaux de sensibilit√© d√©tect√©s: {sensitivity_levels}")
        
        # Mapping des attributs m√©tier pour chaque niveau
        sensitivity_mapping = {
            "PUBLIC": {"risk_level": "LOW", "retention_period": "UNLIMITED", "access_level": "PUBLIC"},
            "INTERNAL": {"risk_level": "LOW", "retention_period": "7_YEARS", "access_level": "INTERNAL"},
            "CONFIDENTIAL": {"risk_level": "MEDIUM", "retention_period": "5_YEARS", "access_level": "RESTRICTED"},
            "PERSONAL_DATA": {"risk_level": "HIGH", "retention_period": "2_YEARS", "access_level": "CONTROLLED"},
            "RESTRICTED": {"risk_level": "CRITICAL", "retention_period": "1_YEAR", "access_level": "HIGHLY_RESTRICTED"}
        }
        
        classification_defs = []
        
        for level in sensitivity_levels:
            attrs = sensitivity_mapping.get(level, {"risk_level": "MEDIUM", "retention_period": "3_YEARS", "access_level": "RESTRICTED"})
            
            classification_def = {
                "name": f"DataSensitivity_{level}",
                "description": f"Classification de sensibilit√© des donn√©es: {level}",
                "attributeDefs": [
                    {"name": "sensitivity_level", "typeName": "string", "isOptional": False},
                    {"name": "risk_level", "typeName": "string", "isOptional": True},
                    {"name": "retention_period", "typeName": "string", "isOptional": True},
                    {"name": "access_level", "typeName": "string", "isOptional": True},
                    {"name": "rgpd_compliant", "typeName": "boolean", "isOptional": True},
                    {"name": "data_steward", "typeName": "string", "isOptional": True},
                    {"name": "validation_date", "typeName": "date", "isOptional": True}
                ]
            }
            classification_defs.append(classification_def)
        
        if classification_defs:
            classification_batch = {"classificationDefs": classification_defs}
            
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/types/typedefs",
                json=classification_batch,
                auth=self.auth
            )
            
            if response.status_code == 200:
                logger.info(f"‚úì {len(classification_defs)} classifications de sensibilit√© cr√©√©es")
                {"success": True, "nonerror": "non echec"}
            else:
                logger.error(f"‚úó Erreur cr√©ation classifications: {response.text}")
                return {"success": False, "error": "√âchec cr√©ation classifications"}
        
        return {"success": False, "error": "√âchec "}


    def automate_hive_glossary_assignment(self, table_name="entites_marocaines"):  
     """Workflow principal d'assignation automatique"""  
     logger.info(f"üöÄ D√©but assignation automatique pour table: {table_name}")  
      
     try:  
        # 1. R√©cup√©rer la table Hive  
        table_guid = self.get_hive_table_entity(table_name)  
        if not table_guid:  
            logger.error(f"‚ùå Table {table_name} non trouv√©e dans Atlas")  
            return {"success": False, "error": "Table non trouv√©e"}  
          
        # 2. R√©cup√©rer les colonnes  
        columns = self.get_table_columns(table_guid)  
        logger.info(f"üìã Colonnes trouv√©es: {[col['name'] for col in columns]}")  
          
        # 3. Cr√©er le mapping colonnes ‚Üí termes  
        column_term_mapping = self.create_column_term_mapping()  
          
        # 4. Assigner les termes aux colonnes  
        assigned_count = 0  
        for column in columns:  
            column_name = column['name']  
              
            if column_name in column_term_mapping:  
                term_name = column_term_mapping[column_name]  
                  
                # R√©cup√©rer le GUID du terme  
                term_guid = self.get_glossary_term_guid(term_name)  
                  
                if term_guid:  
                    success = self.assign_glossary_term_to_column(column['guid'], term_guid)  
                      
                    if success:  
                        logger.info(f"‚úÖ Terme '{term_name}' assign√© √† colonne '{column_name}'")  
                        assigned_count += 1  
                    else:  
                        logger.error(f"‚ùå √âchec assignation pour '{column_name}'")  
                else:  
                    logger.warning(f"‚ö†Ô∏è  Terme '{term_name}' non trouv√© dans glossaire")  
            else:  
                logger.info(f"‚ÑπÔ∏è  Pas de mapping pour colonne '{column_name}'")  
          
        return {  
            "success": True,  
            "table_guid": table_guid,  
            "columns_processed": len(columns),  
            "terms_assigned": assigned_count  
        }  
          
     except Exception as e:  
        logger.error(f"‚ùå Erreur assignation: {str(e)}")  
        return {"success": False, "error": str(e)}
    



    def create_validated_metadata_terms(self, glossary_guid, category_guids):
        """Cr√©er les termes du glossaire √† partir des m√©tadonn√©es valid√©es"""
        enriched_metadata = self.metadata_db['enriched_metadata']
        
        # R√©cup√©rer uniquement les m√©tadonn√©es valid√©es
        validated_metadata = list(enriched_metadata.find({"validation_status": "validated"}))
        logger.info(f"M√©tadonn√©es valid√©es √† synchroniser: {len(validated_metadata)}")
        
        synced_terms = 0
        
        for metadata in validated_metadata:
            column_name = metadata['column_name']
            job_id = metadata['job_id']
            
            # Cr√©er un nom de terme unique et m√©tier
            term_name = f"{column_name.upper()}_TERM"
            qualified_name = f"datagovernance.{column_name}_{job_id}@production"
            
            # Pr√©parer les attributs m√©tier
            attributes = {
                "source_column": column_name,
                "source_dataset": job_id,
                "entity_types": metadata.get('entity_types', []),
                "total_entities": metadata.get('total_entities', 0),
                "sensitivity_level": metadata.get('recommended_sensitivity_level'),
                "rgpd_category": metadata.get('recommended_rgpd_category'),
                "ranger_policy": metadata.get('recommended_ranger_policy'),
                "validation_date": datetime.now().isoformat(),
                "data_quality_score": self._calculate_data_quality_score(metadata),
                "business_owner": "Data Steward",
                "technical_owner": "Data Engineering Team"
            }
            
            # Obtenir la cat√©gorie RGPD
            rgpd_category = metadata.get('recommended_rgpd_category')
            category_guid = category_guids.get(rgpd_category)
            
            # Pr√©parer les classifications
            sensitivity_level = metadata.get('recommended_sensitivity_level')
            classifications = []
            if sensitivity_level:
                classifications.append({
                    "typeName": f"DataSensitivity_{sensitivity_level}",
                    "attributes": {
                        "sensitivity_level": sensitivity_level,
                        "rgpd_compliant": True,
                        "data_steward": "Validated",
                        "validation_date": datetime.now().isoformat()
                    }
                })
            
            term_data = {
                "name": term_name,
                "qualifiedName": qualified_name,
                "shortDescription": f"Attribut m√©tier valid√©: {column_name}",
                "longDescription": self._generate_business_description(metadata),
                "anchor": {"glossaryGuid": glossary_guid}
               
            }
            
            # Ajouter la cat√©gorie RGPD si disponible
            if category_guid:
                term_data["categories"] = [{"categoryGuid": category_guid}]
            
            # Cr√©er le terme dans Atlas
            response = requests.post(
                f"{self.atlas_url}/api/atlas/v2/glossary/term",
                json=term_data,
                auth=self.auth
            )
            
            if response.status_code == 200:
                logger.info(f"‚úì Terme m√©tier synchronis√©: {term_name}")
                synced_terms += 1
            else:
                logger.error(f"‚úó Erreur terme {term_name}: {response.text}")
        
        return synced_terms

    def _calculate_data_quality_score(self, metadata):
        """Calculer un score de qualit√© des donn√©es"""
        score = 0
        
        # Pr√©sence d'entit√©s d√©tect√©es
        if metadata.get('total_entities', 0) > 0:
            score += 30
        
        # Diversit√© des types d'entit√©s
        entity_types = metadata.get('entity_types', [])
        if len(entity_types) > 0:
            score += 20
        
        # Validation par data steward
        if metadata.get('validation_status') == 'validated':
            score += 40
        
        # Pr√©sence d'√©chantillons
        if metadata.get('sample_values') and len(metadata.get('sample_values', [])) > 0:
            score += 10
        
        return min(score, 100)

    def _generate_business_description(self, metadata):
        """G√©n√©rer une description m√©tier riche"""
        column_name = metadata['column_name']
        entity_types = metadata.get('entity_types', [])
        sensitivity = metadata.get('recommended_sensitivity_level', 'INTERNAL')
        rgpd_category = metadata.get('recommended_rgpd_category', 'Non classifi√©')
        total_entities = metadata.get('total_entities', 0)
        
        description = f"""
ATTRIBUT M√âTIER VALID√â: {column_name.upper()}

üîç ANALYSE AUTOMATIQUE:
‚Ä¢ Types d'entit√©s d√©tect√©es: {', '.join(entity_types) if entity_types else 'Aucune entit√© sp√©cifique'}
‚Ä¢ Nombre total d'entit√©s: {total_entities}
‚Ä¢ Niveau de sensibilit√©: {sensitivity}

üìã CLASSIFICATION RGPD:
‚Ä¢ Cat√©gorie: {rgpd_category}
‚Ä¢ Politique Ranger recommand√©e: {metadata.get('recommended_ranger_policy', 'Non d√©finie')}

‚úÖ VALIDATION:
‚Ä¢ Statut: Valid√© par Data Steward
‚Ä¢ Date de validation: {datetime.now().strftime('%Y-%m-%d')}

üìä √âCHANTILLONS:
{self._format_sample_values(metadata.get('sample_values', []))}
        """.strip()
        
        return description

    def _format_sample_values(self, sample_values):
        """Formater les valeurs d'√©chantillon pour la description"""
        if not sample_values:
            return "‚Ä¢ Aucun √©chantillon disponible"
        
        formatted_samples = []
        for i, sample in enumerate(sample_values[:3], 1):  # Limiter √† 3 √©chantillons
            # Masquer partiellement pour la confidentialit√©
            if len(sample) > 10:
                masked_sample = sample[:3] + "***" + sample[-2:]
            else:
                masked_sample = sample[:2] + "***"
            formatted_samples.append(f"‚Ä¢ √âchantillon {i}: {masked_sample}")
        
        return '\n'.join(formatted_samples)

    def create_data_lineage_entities(self):
        """Cr√©er des entit√©s pour la tra√ßabilit√© des donn√©es"""
        # Cette fonction pourrait √™tre √©tendue pour cr√©er des entit√©s Atlas
        # repr√©sentant les datasets, tables, et leurs relations
        logger.info("üîó Fonction de lignage des donn√©es - √Ä impl√©menter selon vos besoins")
        
    def sync_governance_metadata(self, preview_only=False):
        """Fonction principale de synchronisation pour la gouvernance"""
        logger.info("üöÄ D√©but de la synchronisation m√©tadonn√©es gouvernance")
        
        try:
            # Pr√©visualisation obligatoire
            preview = self.preview_sync_data()
            
            if preview_only:
                logger.info("üëÅÔ∏è  Mode pr√©visualisation uniquement - Aucune modification dans Atlas")
                return {"success": True, "preview": preview, "sync_executed": False}
            
            if not preview["will_sync"]:
                logger.warning("üõë Arr√™t: Aucune m√©tadonn√©e valid√©e √† synchroniser")
                return {"success": False, "error": "Aucune m√©tadonn√©e valid√©e", "preview": preview}
            
            # Demander confirmation
            if not self._confirm_sync(preview):
                logger.info("üõë Synchronisation annul√©e par l'utilisateur")
                return {"success": False, "error": "Annul√©e par l'utilisateur", "preview": preview}
            # Demander confirmation
            if not self._confirm_sync(preview):
                logger.info("üõë Synchronisation annul√©e par l'utilisateur")
                return {"success": False, "error": "Annul√©e par l'utilisateur", "preview": preview}
            
            # Sauvegarder l'√©tat actuel d'Atlas (optionnel)
            self._backup_atlas_state()
            
            logger.info("‚ñ∂Ô∏è  EX√âCUTION DE LA SYNCHRONISATION...")
            
            # 1. Cr√©er les classifications de sensibilit√©
            if not self.create_sensitivity_classifications():
                logger.error("√âchec cr√©ation classifications")
                return False
            
            # 2. Cr√©er le glossaire m√©tier
            glossary_guid = self.create_business_glossary()
            if not glossary_guid:
                logger.error("√âchec cr√©ation glossaire")
                return False
            
            # 3. Cr√©er les cat√©gories RGPD bas√©es sur les donn√©es r√©elles
            category_guids = self.create_rgpd_categories(glossary_guid)
            
            # 4. Synchroniser uniquement les m√©tadonn√©es valid√©es
            synced_terms = self.create_validated_metadata_terms(glossary_guid, category_guids)
            
            # 5. Cr√©er la lignage (optionnel)
            self.create_data_lineage_entities()
            # 6. Assigner automatiquement les termes aux colonnes Hive  
            assignment_result = self.automate_hive_glossary_assignment("entites_marocaines")  
   
            result = {  
             "success": True,  
             "glossary_guid": glossary_guid,  
             "validated_terms_synced": synced_terms,  
             "categories_created": len(category_guids),  
             "sync_timestamp": datetime.now().isoformat(),  
             "preview": preview  
            }  
  





  
            result.update({  
              "hive_assignment": assignment_result  
            })
            
            # Marquer les m√©tadonn√©es comme synchronis√©es
            self._mark_as_synced(synced_terms)
            
            result = {
                "success": True,
                "glossary_guid": glossary_guid,
                "validated_terms_synced": synced_terms,
                "categories_created": len(category_guids),
                "sync_timestamp": datetime.now().isoformat(),
                "preview": preview
            }
            
            logger.info(f"‚úÖ Synchronisation termin√©e avec succ√®s: {synced_terms} termes valid√©s")
            return result
            
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de la synchronisation: {str(e)}")
            return {"success": False, "error": str(e)}
        
        finally:
            self.mongo_client.close()

    def _confirm_sync(self, preview):
        """Demander confirmation avant synchronisation"""
        print("\n" + "="*60)
        print("‚ö†Ô∏è  CONFIRMATION REQUISE")
        print("="*60)
        print(f"Vous allez synchroniser {preview['validated_metadata']} m√©tadonn√©es valid√©es vers Atlas")
        print(f"Cat√©gories RGPD: {', '.join(preview['rgpd_categories'])}")
        print(f"Niveaux de sensibilit√©: {', '.join(preview['sensitivity_levels'])}")
        print("\n‚ö†Ô∏è  Cette op√©ration va cr√©er/modifier des √©l√©ments dans Apache Atlas")
        
        response = input("\nü§î Continuer la synchronisation? (oui/non): ").lower().strip()
        return response in ['oui', 'o', 'yes', 'y']
    
    def _backup_atlas_state(self):
        """Sauvegarder l'√©tat actuel d'Atlas (optionnel)"""
        logger.info("üíæ Sauvegarde de l'√©tat Atlas (recommand√©)")
        # Impl√©mentation optionnelle pour sauvegarder l'√©tat actuel
        pass
    
    def _mark_as_synced(self, synced_count):
        """Marquer les m√©tadonn√©es synchronis√©es"""
        if synced_count > 0:
            enriched_metadata = self.metadata_db['enriched_metadata']
            enriched_metadata.update_many(
                {"validation_status": "validated"},
                {"$set": {"atlas_sync_status": "synced", "atlas_sync_date": datetime.now()}}
            )
            logger.info(f"üìù {synced_count} m√©tadonn√©es marqu√©es comme synchronis√©es")

def main():
    """Point d'entr√©e principal"""
    governance = AtlasMetadataGovernance()
    result = governance.sync_governance_metadata()
    
    print("\n" + "="*60)
    print("R√âSULTAT DE LA SYNCHRONISATION GOUVERNANCE")
    print("="*60)
    print(json.dumps(result, indent=2, ensure_ascii=False))
    
    if result.get("success"):
        print(f"\n‚úÖ Synchronisation r√©ussie!")
        print(f"üìö Termes valid√©s synchronis√©s: {result.get('validated_terms_synced', 0)}")
        print(f"üìÇ Cat√©gories RGPD cr√©√©es: {result.get('categories_created', 0)}")
        print(f"üÜî GUID du glossaire: {result.get('glossary_guid')}")
    else:
        print(f"\n‚ùå √âchec de la synchronisation: {result.get('error')}")

if __name__ == "__main__":
    main()